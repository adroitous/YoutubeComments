<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_eg70cn4trppt-1>li:before{content:"" counter(lst-ctn-kix_eg70cn4trppt-1,lower-latin) ". "}ol.lst-kix_6myvsjmn8qag-4.start{counter-reset:lst-ctn-kix_6myvsjmn8qag-4 0}ol.lst-kix_g3dvsik8v46c-2.start{counter-reset:lst-ctn-kix_g3dvsik8v46c-2 0}.lst-kix_eg70cn4trppt-3>li:before{content:"" counter(lst-ctn-kix_eg70cn4trppt-3,decimal) ". "}.lst-kix_eg70cn4trppt-2>li:before{content:"" counter(lst-ctn-kix_eg70cn4trppt-2,lower-roman) ". "}ol.lst-kix_67q6k7r76ppj-1.start{counter-reset:lst-ctn-kix_67q6k7r76ppj-1 0}ul.lst-kix_3wy0zjmzx4j3-1{list-style-type:none}ul.lst-kix_3wy0zjmzx4j3-0{list-style-type:none}ul.lst-kix_3wy0zjmzx4j3-3{list-style-type:none}.lst-kix_eg70cn4trppt-0>li:before{content:"" counter(lst-ctn-kix_eg70cn4trppt-0,decimal) ". "}.lst-kix_eg70cn4trppt-8>li:before{content:"" counter(lst-ctn-kix_eg70cn4trppt-8,lower-roman) ". "}ul.lst-kix_3wy0zjmzx4j3-2{list-style-type:none}ol.lst-kix_95tx3dl9s8o8-3{list-style-type:none}.lst-kix_eg70cn4trppt-4>li{counter-increment:lst-ctn-kix_eg70cn4trppt-4}ol.lst-kix_95tx3dl9s8o8-4{list-style-type:none}ol.lst-kix_95tx3dl9s8o8-5{list-style-type:none}ol.lst-kix_95tx3dl9s8o8-6{list-style-type:none}ol.lst-kix_95tx3dl9s8o8-7{list-style-type:none}ol.lst-kix_95tx3dl9s8o8-8{list-style-type:none}.lst-kix_doipf27yxj47-0>li:before{content:"" counter(lst-ctn-kix_doipf27yxj47-0,decimal) ". "}.lst-kix_doipf27yxj47-1>li:before{content:"" counter(lst-ctn-kix_doipf27yxj47-1,lower-latin) ". "}.lst-kix_67q6k7r76ppj-5>li:before{content:"" counter(lst-ctn-kix_67q6k7r76ppj-5,lower-roman) ". "}.lst-kix_g3dvsik8v46c-7>li{counter-increment:lst-ctn-kix_g3dvsik8v46c-7}ol.lst-kix_95tx3dl9s8o8-0{list-style-type:none}ol.lst-kix_95tx3dl9s8o8-1{list-style-type:none}ol.lst-kix_95tx3dl9s8o8-1.start{counter-reset:lst-ctn-kix_95tx3dl9s8o8-1 0}ol.lst-kix_5orfu8u579kf-4.start{counter-reset:lst-ctn-kix_5orfu8u579kf-4 0}ol.lst-kix_95tx3dl9s8o8-2{list-style-type:none}.lst-kix_95tx3dl9s8o8-3>li{counter-increment:lst-ctn-kix_95tx3dl9s8o8-3}.lst-kix_67q6k7r76ppj-6>li:before{content:"" counter(lst-ctn-kix_67q6k7r76ppj-6,decimal) ". "}.lst-kix_67q6k7r76ppj-7>li{counter-increment:lst-ctn-kix_67q6k7r76ppj-7}.lst-kix_n02yq4vl24pk-1>li{counter-increment:lst-ctn-kix_n02yq4vl24pk-1}.lst-kix_doipf27yxj47-4>li:before{content:"" counter(lst-ctn-kix_doipf27yxj47-4,lower-latin) ". "}.lst-kix_doipf27yxj47-5>li:before{content:"" counter(lst-ctn-kix_doipf27yxj47-5,lower-roman) ". "}.lst-kix_67q6k7r76ppj-7>li:before{content:"" counter(lst-ctn-kix_67q6k7r76ppj-7,lower-latin) ". "}.lst-kix_doipf27yxj47-2>li:before{content:"" counter(lst-ctn-kix_doipf27yxj47-2,lower-roman) ". "}.lst-kix_doipf27yxj47-6>li:before{content:"" counter(lst-ctn-kix_doipf27yxj47-6,decimal) ". "}ol.lst-kix_n02yq4vl24pk-0.start{counter-reset:lst-ctn-kix_n02yq4vl24pk-0 0}.lst-kix_67q6k7r76ppj-8>li:before{content:"" counter(lst-ctn-kix_67q6k7r76ppj-8,lower-roman) ". "}.lst-kix_doipf27yxj47-3>li:before{content:"" counter(lst-ctn-kix_doipf27yxj47-3,decimal) ". "}ol.lst-kix_eg70cn4trppt-4.start{counter-reset:lst-ctn-kix_eg70cn4trppt-4 0}.lst-kix_eg70cn4trppt-7>li:before{content:"" counter(lst-ctn-kix_eg70cn4trppt-7,lower-latin) ". "}.lst-kix_eg70cn4trppt-6>li:before{content:"" counter(lst-ctn-kix_eg70cn4trppt-6,decimal) ". "}ol.lst-kix_doipf27yxj47-5.start{counter-reset:lst-ctn-kix_doipf27yxj47-5 0}.lst-kix_doipf27yxj47-8>li{counter-increment:lst-ctn-kix_doipf27yxj47-8}.lst-kix_eg70cn4trppt-5>li:before{content:"" counter(lst-ctn-kix_eg70cn4trppt-5,lower-roman) ". "}.lst-kix_doipf27yxj47-8>li:before{content:"" counter(lst-ctn-kix_doipf27yxj47-8,lower-roman) ". "}.lst-kix_eg70cn4trppt-2>li{counter-increment:lst-ctn-kix_eg70cn4trppt-2}.lst-kix_doipf27yxj47-7>li:before{content:"" counter(lst-ctn-kix_doipf27yxj47-7,lower-latin) ". "}ol.lst-kix_g3dvsik8v46c-7.start{counter-reset:lst-ctn-kix_g3dvsik8v46c-7 0}.lst-kix_eg70cn4trppt-4>li:before{content:"" counter(lst-ctn-kix_eg70cn4trppt-4,lower-latin) ". "}.lst-kix_95tx3dl9s8o8-1>li{counter-increment:lst-ctn-kix_95tx3dl9s8o8-1}.lst-kix_hwyt9y26yq4r-0>li:before{content:"\0025cf  "}ol.lst-kix_5orfu8u579kf-3.start{counter-reset:lst-ctn-kix_5orfu8u579kf-3 0}.lst-kix_67q6k7r76ppj-5>li{counter-increment:lst-ctn-kix_67q6k7r76ppj-5}.lst-kix_hwyt9y26yq4r-3>li:before{content:"\0025cf  "}.lst-kix_hwyt9y26yq4r-1>li:before{content:"\0025cb  "}.lst-kix_hwyt9y26yq4r-2>li:before{content:"\0025a0  "}.lst-kix_95tx3dl9s8o8-7>li{counter-increment:lst-ctn-kix_95tx3dl9s8o8-7}ol.lst-kix_doipf27yxj47-4.start{counter-reset:lst-ctn-kix_doipf27yxj47-4 0}ol.lst-kix_67q6k7r76ppj-6.start{counter-reset:lst-ctn-kix_67q6k7r76ppj-6 0}.lst-kix_6myvsjmn8qag-1>li{counter-increment:lst-ctn-kix_6myvsjmn8qag-1}.lst-kix_n02yq4vl24pk-3>li{counter-increment:lst-ctn-kix_n02yq4vl24pk-3}ol.lst-kix_6myvsjmn8qag-5.start{counter-reset:lst-ctn-kix_6myvsjmn8qag-5 0}ol.lst-kix_g3dvsik8v46c-3.start{counter-reset:lst-ctn-kix_g3dvsik8v46c-3 0}.lst-kix_5orfu8u579kf-1>li{counter-increment:lst-ctn-kix_5orfu8u579kf-1}ol.lst-kix_g3dvsik8v46c-8.start{counter-reset:lst-ctn-kix_g3dvsik8v46c-8 0}.lst-kix_67q6k7r76ppj-4>li:before{content:"" counter(lst-ctn-kix_67q6k7r76ppj-4,lower-latin) ". "}ol.lst-kix_67q6k7r76ppj-7.start{counter-reset:lst-ctn-kix_67q6k7r76ppj-7 0}.lst-kix_67q6k7r76ppj-3>li:before{content:"" counter(lst-ctn-kix_67q6k7r76ppj-3,decimal) ". "}.lst-kix_eg70cn4trppt-6>li{counter-increment:lst-ctn-kix_eg70cn4trppt-6}.lst-kix_67q6k7r76ppj-2>li:before{content:"" counter(lst-ctn-kix_67q6k7r76ppj-2,lower-roman) ". "}ul.lst-kix_3wy0zjmzx4j3-5{list-style-type:none}.lst-kix_hwyt9y26yq4r-7>li:before{content:"\0025cb  "}.lst-kix_hwyt9y26yq4r-8>li:before{content:"\0025a0  "}.lst-kix_doipf27yxj47-1>li{counter-increment:lst-ctn-kix_doipf27yxj47-1}.lst-kix_doipf27yxj47-4>li{counter-increment:lst-ctn-kix_doipf27yxj47-4}ul.lst-kix_3wy0zjmzx4j3-4{list-style-type:none}ol.lst-kix_67q6k7r76ppj-0.start{counter-reset:lst-ctn-kix_67q6k7r76ppj-0 0}ul.lst-kix_3wy0zjmzx4j3-7{list-style-type:none}.lst-kix_67q6k7r76ppj-1>li:before{content:"" counter(lst-ctn-kix_67q6k7r76ppj-1,lower-latin) ". "}ul.lst-kix_3wy0zjmzx4j3-6{list-style-type:none}ul.lst-kix_3wy0zjmzx4j3-8{list-style-type:none}.lst-kix_67q6k7r76ppj-0>li:before{content:"" counter(lst-ctn-kix_67q6k7r76ppj-0,decimal) ". "}.lst-kix_hwyt9y26yq4r-4>li:before{content:"\0025cb  "}.lst-kix_hwyt9y26yq4r-5>li:before{content:"\0025a0  "}.lst-kix_hwyt9y26yq4r-6>li:before{content:"\0025cf  "}ol.lst-kix_95tx3dl9s8o8-5.start{counter-reset:lst-ctn-kix_95tx3dl9s8o8-5 0}.lst-kix_67q6k7r76ppj-2>li{counter-increment:lst-ctn-kix_67q6k7r76ppj-2}.lst-kix_6myvsjmn8qag-5>li{counter-increment:lst-ctn-kix_6myvsjmn8qag-5}ol.lst-kix_5orfu8u579kf-0{list-style-type:none}ol.lst-kix_5orfu8u579kf-1{list-style-type:none}ol.lst-kix_5orfu8u579kf-2{list-style-type:none}ol.lst-kix_5orfu8u579kf-3{list-style-type:none}ol.lst-kix_5orfu8u579kf-4{list-style-type:none}ol.lst-kix_eg70cn4trppt-2.start{counter-reset:lst-ctn-kix_eg70cn4trppt-2 0}ol.lst-kix_5orfu8u579kf-5{list-style-type:none}ol.lst-kix_5orfu8u579kf-6{list-style-type:none}ol.lst-kix_5orfu8u579kf-7{list-style-type:none}ol.lst-kix_5orfu8u579kf-8{list-style-type:none}.lst-kix_g3dvsik8v46c-1>li{counter-increment:lst-ctn-kix_g3dvsik8v46c-1}.lst-kix_6myvsjmn8qag-4>li{counter-increment:lst-ctn-kix_6myvsjmn8qag-4}ol.lst-kix_eg70cn4trppt-5.start{counter-reset:lst-ctn-kix_eg70cn4trppt-5 0}.lst-kix_67q6k7r76ppj-1>li{counter-increment:lst-ctn-kix_67q6k7r76ppj-1}.lst-kix_eg70cn4trppt-0>li{counter-increment:lst-ctn-kix_eg70cn4trppt-0}ol.lst-kix_6myvsjmn8qag-0.start{counter-reset:lst-ctn-kix_6myvsjmn8qag-0 0}.lst-kix_6myvsjmn8qag-4>li:before{content:"" counter(lst-ctn-kix_6myvsjmn8qag-4,lower-latin) ". "}ol.lst-kix_n02yq4vl24pk-1.start{counter-reset:lst-ctn-kix_n02yq4vl24pk-1 0}ol.lst-kix_6myvsjmn8qag-5{list-style-type:none}ol.lst-kix_6myvsjmn8qag-4{list-style-type:none}ol.lst-kix_6myvsjmn8qag-7{list-style-type:none}ol.lst-kix_6myvsjmn8qag-6{list-style-type:none}ol.lst-kix_6myvsjmn8qag-8{list-style-type:none}.lst-kix_n02yq4vl24pk-7>li{counter-increment:lst-ctn-kix_n02yq4vl24pk-7}.lst-kix_6myvsjmn8qag-6>li:before{content:"" counter(lst-ctn-kix_6myvsjmn8qag-6,decimal) ". "}ol.lst-kix_5orfu8u579kf-2.start{counter-reset:lst-ctn-kix_5orfu8u579kf-2 0}.lst-kix_67q6k7r76ppj-0>li{counter-increment:lst-ctn-kix_67q6k7r76ppj-0}.lst-kix_6myvsjmn8qag-6>li{counter-increment:lst-ctn-kix_6myvsjmn8qag-6}ol.lst-kix_6myvsjmn8qag-1{list-style-type:none}ol.lst-kix_6myvsjmn8qag-3.start{counter-reset:lst-ctn-kix_6myvsjmn8qag-3 0}.lst-kix_6myvsjmn8qag-8>li:before{content:"" counter(lst-ctn-kix_6myvsjmn8qag-8,lower-roman) ". "}ol.lst-kix_6myvsjmn8qag-0{list-style-type:none}.lst-kix_g3dvsik8v46c-3>li{counter-increment:lst-ctn-kix_g3dvsik8v46c-3}ol.lst-kix_6myvsjmn8qag-3{list-style-type:none}ol.lst-kix_6myvsjmn8qag-2{list-style-type:none}ol.lst-kix_95tx3dl9s8o8-8.start{counter-reset:lst-ctn-kix_95tx3dl9s8o8-8 0}ol.lst-kix_g3dvsik8v46c-4.start{counter-reset:lst-ctn-kix_g3dvsik8v46c-4 0}.lst-kix_5orfu8u579kf-3>li{counter-increment:lst-ctn-kix_5orfu8u579kf-3}.lst-kix_eg70cn4trppt-8>li{counter-increment:lst-ctn-kix_eg70cn4trppt-8}.lst-kix_doipf27yxj47-5>li{counter-increment:lst-ctn-kix_doipf27yxj47-5}.lst-kix_5orfu8u579kf-1>li:before{content:"" counter(lst-ctn-kix_5orfu8u579kf-1,lower-latin) ". "}ol.lst-kix_67q6k7r76ppj-8.start{counter-reset:lst-ctn-kix_67q6k7r76ppj-8 0}.lst-kix_5orfu8u579kf-2>li{counter-increment:lst-ctn-kix_5orfu8u579kf-2}.lst-kix_5orfu8u579kf-8>li{counter-increment:lst-ctn-kix_5orfu8u579kf-8}.lst-kix_doipf27yxj47-6>li{counter-increment:lst-ctn-kix_doipf27yxj47-6}ol.lst-kix_95tx3dl9s8o8-7.start{counter-reset:lst-ctn-kix_95tx3dl9s8o8-7 0}ol.lst-kix_6myvsjmn8qag-2.start{counter-reset:lst-ctn-kix_6myvsjmn8qag-2 0}.lst-kix_5orfu8u579kf-3>li:before{content:"" counter(lst-ctn-kix_5orfu8u579kf-3,decimal) ". "}.lst-kix_g3dvsik8v46c-5>li:before{content:"" counter(lst-ctn-kix_g3dvsik8v46c-5,lower-roman) ". "}.lst-kix_g3dvsik8v46c-7>li:before{content:"" counter(lst-ctn-kix_g3dvsik8v46c-7,lower-latin) ". "}.lst-kix_5orfu8u579kf-5>li:before{content:"" counter(lst-ctn-kix_5orfu8u579kf-5,lower-roman) ". "}.lst-kix_5orfu8u579kf-7>li:before{content:"" counter(lst-ctn-kix_5orfu8u579kf-7,lower-latin) ". "}ol.lst-kix_doipf27yxj47-1.start{counter-reset:lst-ctn-kix_doipf27yxj47-1 0}.lst-kix_eg70cn4trppt-7>li{counter-increment:lst-ctn-kix_eg70cn4trppt-7}.lst-kix_doipf27yxj47-0>li{counter-increment:lst-ctn-kix_doipf27yxj47-0}ol.lst-kix_eg70cn4trppt-3.start{counter-reset:lst-ctn-kix_eg70cn4trppt-3 0}ol.lst-kix_95tx3dl9s8o8-6.start{counter-reset:lst-ctn-kix_95tx3dl9s8o8-6 0}.lst-kix_6myvsjmn8qag-2>li:before{content:"" counter(lst-ctn-kix_6myvsjmn8qag-2,lower-roman) ". "}ol.lst-kix_g3dvsik8v46c-6.start{counter-reset:lst-ctn-kix_g3dvsik8v46c-6 0}.lst-kix_95tx3dl9s8o8-2>li{counter-increment:lst-ctn-kix_95tx3dl9s8o8-2}.lst-kix_6myvsjmn8qag-0>li:before{content:"" counter(lst-ctn-kix_6myvsjmn8qag-0,decimal) ". "}.lst-kix_eg70cn4trppt-1>li{counter-increment:lst-ctn-kix_eg70cn4trppt-1}ol.lst-kix_6myvsjmn8qag-1.start{counter-reset:lst-ctn-kix_6myvsjmn8qag-1 0}ol.lst-kix_doipf27yxj47-0.start{counter-reset:lst-ctn-kix_doipf27yxj47-0 0}ol.lst-kix_5orfu8u579kf-0.start{counter-reset:lst-ctn-kix_5orfu8u579kf-0 0}.lst-kix_n02yq4vl24pk-2>li{counter-increment:lst-ctn-kix_n02yq4vl24pk-2}.lst-kix_n02yq4vl24pk-8>li{counter-increment:lst-ctn-kix_n02yq4vl24pk-8}.lst-kix_g3dvsik8v46c-2>li{counter-increment:lst-ctn-kix_g3dvsik8v46c-2}.lst-kix_g3dvsik8v46c-3>li:before{content:"" counter(lst-ctn-kix_g3dvsik8v46c-3,decimal) ". "}.lst-kix_95tx3dl9s8o8-8>li{counter-increment:lst-ctn-kix_95tx3dl9s8o8-8}.lst-kix_g3dvsik8v46c-1>li:before{content:"" counter(lst-ctn-kix_g3dvsik8v46c-1,lower-latin) ". "}.lst-kix_95tx3dl9s8o8-4>li{counter-increment:lst-ctn-kix_95tx3dl9s8o8-4}ol.lst-kix_n02yq4vl24pk-8.start{counter-reset:lst-ctn-kix_n02yq4vl24pk-8 0}.lst-kix_67q6k7r76ppj-8>li{counter-increment:lst-ctn-kix_67q6k7r76ppj-8}.lst-kix_g3dvsik8v46c-8>li{counter-increment:lst-ctn-kix_g3dvsik8v46c-8}ol.lst-kix_6myvsjmn8qag-7.start{counter-reset:lst-ctn-kix_6myvsjmn8qag-7 0}ol.lst-kix_g3dvsik8v46c-5.start{counter-reset:lst-ctn-kix_g3dvsik8v46c-5 0}.lst-kix_3wy0zjmzx4j3-6>li:before{content:"\0025cf  "}.lst-kix_3wy0zjmzx4j3-7>li:before{content:"\0025cb  "}ol.lst-kix_5orfu8u579kf-1.start{counter-reset:lst-ctn-kix_5orfu8u579kf-1 0}.lst-kix_95tx3dl9s8o8-3>li:before{content:"(" counter(lst-ctn-kix_95tx3dl9s8o8-3,decimal) ") "}ol.lst-kix_n02yq4vl24pk-7{list-style-type:none}ol.lst-kix_n02yq4vl24pk-6{list-style-type:none}ol.lst-kix_n02yq4vl24pk-5{list-style-type:none}.lst-kix_95tx3dl9s8o8-2>li:before{content:"" counter(lst-ctn-kix_95tx3dl9s8o8-2,lower-roman) ") "}ol.lst-kix_n02yq4vl24pk-4{list-style-type:none}ol.lst-kix_n02yq4vl24pk-3{list-style-type:none}ol.lst-kix_n02yq4vl24pk-2{list-style-type:none}.lst-kix_3wy0zjmzx4j3-8>li:before{content:"\0025a0  "}.lst-kix_n02yq4vl24pk-0>li{counter-increment:lst-ctn-kix_n02yq4vl24pk-0}ol.lst-kix_n02yq4vl24pk-1{list-style-type:none}.lst-kix_95tx3dl9s8o8-1>li:before{content:"" counter(lst-ctn-kix_95tx3dl9s8o8-1,lower-latin) ") "}ol.lst-kix_n02yq4vl24pk-0{list-style-type:none}ol.lst-kix_67q6k7r76ppj-4.start{counter-reset:lst-ctn-kix_67q6k7r76ppj-4 0}ol.lst-kix_eg70cn4trppt-1.start{counter-reset:lst-ctn-kix_eg70cn4trppt-1 0}.lst-kix_g3dvsik8v46c-6>li{counter-increment:lst-ctn-kix_g3dvsik8v46c-6}.lst-kix_95tx3dl9s8o8-0>li:before{content:"" counter(lst-ctn-kix_95tx3dl9s8o8-0,decimal) ") "}ol.lst-kix_doipf27yxj47-8.start{counter-reset:lst-ctn-kix_doipf27yxj47-8 0}.lst-kix_6myvsjmn8qag-0>li{counter-increment:lst-ctn-kix_6myvsjmn8qag-0}.lst-kix_5orfu8u579kf-0>li{counter-increment:lst-ctn-kix_5orfu8u579kf-0}.lst-kix_67q6k7r76ppj-6>li{counter-increment:lst-ctn-kix_67q6k7r76ppj-6}ol.lst-kix_67q6k7r76ppj-8{list-style-type:none}ol.lst-kix_67q6k7r76ppj-4{list-style-type:none}ol.lst-kix_67q6k7r76ppj-5{list-style-type:none}ol.lst-kix_67q6k7r76ppj-6{list-style-type:none}ol.lst-kix_67q6k7r76ppj-7{list-style-type:none}ol.lst-kix_67q6k7r76ppj-0{list-style-type:none}ol.lst-kix_doipf27yxj47-2.start{counter-reset:lst-ctn-kix_doipf27yxj47-2 0}ol.lst-kix_67q6k7r76ppj-1{list-style-type:none}ol.lst-kix_67q6k7r76ppj-2{list-style-type:none}ol.lst-kix_67q6k7r76ppj-3{list-style-type:none}.lst-kix_eg70cn4trppt-5>li{counter-increment:lst-ctn-kix_eg70cn4trppt-5}.lst-kix_n02yq4vl24pk-4>li:before{content:"" counter(lst-ctn-kix_n02yq4vl24pk-4,lower-latin) ". "}.lst-kix_n02yq4vl24pk-5>li:before{content:"" counter(lst-ctn-kix_n02yq4vl24pk-5,lower-roman) ". "}ol.lst-kix_eg70cn4trppt-0.start{counter-reset:lst-ctn-kix_eg70cn4trppt-0 0}ol.lst-kix_eg70cn4trppt-7.start{counter-reset:lst-ctn-kix_eg70cn4trppt-7 0}.lst-kix_n02yq4vl24pk-0>li:before{content:"" counter(lst-ctn-kix_n02yq4vl24pk-0,decimal) ". "}.lst-kix_n02yq4vl24pk-8>li:before{content:"" counter(lst-ctn-kix_n02yq4vl24pk-8,lower-roman) ". "}ol.lst-kix_95tx3dl9s8o8-3.start{counter-reset:lst-ctn-kix_95tx3dl9s8o8-3 0}.lst-kix_n02yq4vl24pk-6>li:before{content:"" counter(lst-ctn-kix_n02yq4vl24pk-6,decimal) ". "}.lst-kix_n02yq4vl24pk-7>li:before{content:"" counter(lst-ctn-kix_n02yq4vl24pk-7,lower-latin) ". "}.lst-kix_6myvsjmn8qag-7>li{counter-increment:lst-ctn-kix_6myvsjmn8qag-7}ol.lst-kix_n02yq4vl24pk-3.start{counter-reset:lst-ctn-kix_n02yq4vl24pk-3 0}.lst-kix_g3dvsik8v46c-4>li{counter-increment:lst-ctn-kix_g3dvsik8v46c-4}.lst-kix_95tx3dl9s8o8-6>li{counter-increment:lst-ctn-kix_95tx3dl9s8o8-6}.lst-kix_5orfu8u579kf-7>li{counter-increment:lst-ctn-kix_5orfu8u579kf-7}.lst-kix_n02yq4vl24pk-1>li:before{content:"" counter(lst-ctn-kix_n02yq4vl24pk-1,lower-latin) ". "}.lst-kix_95tx3dl9s8o8-0>li{counter-increment:lst-ctn-kix_95tx3dl9s8o8-0}.lst-kix_n02yq4vl24pk-2>li:before{content:"" counter(lst-ctn-kix_n02yq4vl24pk-2,lower-roman) ". "}.lst-kix_n02yq4vl24pk-3>li:before{content:"" counter(lst-ctn-kix_n02yq4vl24pk-3,decimal) ". "}ol.lst-kix_doipf27yxj47-3.start{counter-reset:lst-ctn-kix_doipf27yxj47-3 0}.lst-kix_eg70cn4trppt-3>li{counter-increment:lst-ctn-kix_eg70cn4trppt-3}ol.lst-kix_n02yq4vl24pk-8{list-style-type:none}.lst-kix_3wy0zjmzx4j3-3>li:before{content:"\0025cf  "}ol.lst-kix_95tx3dl9s8o8-4.start{counter-reset:lst-ctn-kix_95tx3dl9s8o8-4 0}.lst-kix_95tx3dl9s8o8-4>li:before{content:"(" counter(lst-ctn-kix_95tx3dl9s8o8-4,lower-latin) ") "}ol.lst-kix_doipf27yxj47-1{list-style-type:none}ol.lst-kix_doipf27yxj47-0{list-style-type:none}.lst-kix_doipf27yxj47-7>li{counter-increment:lst-ctn-kix_doipf27yxj47-7}.lst-kix_3wy0zjmzx4j3-4>li:before{content:"\0025cb  "}.lst-kix_3wy0zjmzx4j3-5>li:before{content:"\0025a0  "}ol.lst-kix_eg70cn4trppt-6.start{counter-reset:lst-ctn-kix_eg70cn4trppt-6 0}.lst-kix_95tx3dl9s8o8-5>li:before{content:"(" counter(lst-ctn-kix_95tx3dl9s8o8-5,lower-roman) ") "}ol.lst-kix_doipf27yxj47-5{list-style-type:none}ol.lst-kix_doipf27yxj47-4{list-style-type:none}ol.lst-kix_doipf27yxj47-3{list-style-type:none}.lst-kix_95tx3dl9s8o8-6>li:before{content:"" counter(lst-ctn-kix_95tx3dl9s8o8-6,decimal) ". "}.lst-kix_95tx3dl9s8o8-8>li:before{content:"" counter(lst-ctn-kix_95tx3dl9s8o8-8,lower-roman) ". "}ol.lst-kix_doipf27yxj47-2{list-style-type:none}ol.lst-kix_doipf27yxj47-8{list-style-type:none}ol.lst-kix_doipf27yxj47-7{list-style-type:none}.lst-kix_95tx3dl9s8o8-7>li:before{content:"" counter(lst-ctn-kix_95tx3dl9s8o8-7,lower-latin) ". "}ol.lst-kix_doipf27yxj47-6{list-style-type:none}ol.lst-kix_n02yq4vl24pk-2.start{counter-reset:lst-ctn-kix_n02yq4vl24pk-2 0}.lst-kix_3wy0zjmzx4j3-2>li:before{content:"\0025a0  "}.lst-kix_3wy0zjmzx4j3-0>li:before{content:"\0025cf  "}.lst-kix_3wy0zjmzx4j3-1>li:before{content:"\0025cb  "}ol.lst-kix_5orfu8u579kf-5.start{counter-reset:lst-ctn-kix_5orfu8u579kf-5 0}.lst-kix_5orfu8u579kf-5>li{counter-increment:lst-ctn-kix_5orfu8u579kf-5}.lst-kix_5orfu8u579kf-4>li{counter-increment:lst-ctn-kix_5orfu8u579kf-4}ol.lst-kix_n02yq4vl24pk-4.start{counter-reset:lst-ctn-kix_n02yq4vl24pk-4 0}.lst-kix_n02yq4vl24pk-6>li{counter-increment:lst-ctn-kix_n02yq4vl24pk-6}ol.lst-kix_6myvsjmn8qag-8.start{counter-reset:lst-ctn-kix_6myvsjmn8qag-8 0}ol.lst-kix_eg70cn4trppt-8.start{counter-reset:lst-ctn-kix_eg70cn4trppt-8 0}.lst-kix_doipf27yxj47-3>li{counter-increment:lst-ctn-kix_doipf27yxj47-3}.lst-kix_6myvsjmn8qag-3>li:before{content:"" counter(lst-ctn-kix_6myvsjmn8qag-3,decimal) ". "}ol.lst-kix_doipf27yxj47-6.start{counter-reset:lst-ctn-kix_doipf27yxj47-6 0}.lst-kix_n02yq4vl24pk-4>li{counter-increment:lst-ctn-kix_n02yq4vl24pk-4}ol.lst-kix_g3dvsik8v46c-1.start{counter-reset:lst-ctn-kix_g3dvsik8v46c-1 0}ol.lst-kix_n02yq4vl24pk-7.start{counter-reset:lst-ctn-kix_n02yq4vl24pk-7 0}.lst-kix_doipf27yxj47-2>li{counter-increment:lst-ctn-kix_doipf27yxj47-2}.lst-kix_g3dvsik8v46c-0>li{counter-increment:lst-ctn-kix_g3dvsik8v46c-0}.lst-kix_6myvsjmn8qag-5>li:before{content:"" counter(lst-ctn-kix_6myvsjmn8qag-5,lower-roman) ". "}.lst-kix_6myvsjmn8qag-3>li{counter-increment:lst-ctn-kix_6myvsjmn8qag-3}ol.lst-kix_67q6k7r76ppj-2.start{counter-reset:lst-ctn-kix_67q6k7r76ppj-2 0}.lst-kix_6myvsjmn8qag-7>li:before{content:"" counter(lst-ctn-kix_6myvsjmn8qag-7,lower-latin) ". "}.lst-kix_67q6k7r76ppj-3>li{counter-increment:lst-ctn-kix_67q6k7r76ppj-3}ol.lst-kix_95tx3dl9s8o8-2.start{counter-reset:lst-ctn-kix_95tx3dl9s8o8-2 0}ol.lst-kix_6myvsjmn8qag-6.start{counter-reset:lst-ctn-kix_6myvsjmn8qag-6 0}ol.lst-kix_67q6k7r76ppj-5.start{counter-reset:lst-ctn-kix_67q6k7r76ppj-5 0}ol.lst-kix_5orfu8u579kf-8.start{counter-reset:lst-ctn-kix_5orfu8u579kf-8 0}.lst-kix_5orfu8u579kf-6>li{counter-increment:lst-ctn-kix_5orfu8u579kf-6}.lst-kix_5orfu8u579kf-0>li:before{content:"" counter(lst-ctn-kix_5orfu8u579kf-0,decimal) ". "}.lst-kix_g3dvsik8v46c-0>li:before{content:"" counter(lst-ctn-kix_g3dvsik8v46c-0,decimal) ". "}.lst-kix_6myvsjmn8qag-2>li{counter-increment:lst-ctn-kix_6myvsjmn8qag-2}ol.lst-kix_n02yq4vl24pk-6.start{counter-reset:lst-ctn-kix_n02yq4vl24pk-6 0}ol.lst-kix_95tx3dl9s8o8-0.start{counter-reset:lst-ctn-kix_95tx3dl9s8o8-0 0}.lst-kix_5orfu8u579kf-2>li:before{content:"" counter(lst-ctn-kix_5orfu8u579kf-2,lower-roman) ". "}ol.lst-kix_67q6k7r76ppj-3.start{counter-reset:lst-ctn-kix_67q6k7r76ppj-3 0}.lst-kix_g3dvsik8v46c-4>li:before{content:"" counter(lst-ctn-kix_g3dvsik8v46c-4,lower-latin) ". "}ol.lst-kix_g3dvsik8v46c-0.start{counter-reset:lst-ctn-kix_g3dvsik8v46c-0 0}.lst-kix_6myvsjmn8qag-8>li{counter-increment:lst-ctn-kix_6myvsjmn8qag-8}.lst-kix_g3dvsik8v46c-6>li:before{content:"" counter(lst-ctn-kix_g3dvsik8v46c-6,decimal) ". "}.lst-kix_5orfu8u579kf-4>li:before{content:"" counter(lst-ctn-kix_5orfu8u579kf-4,lower-latin) ". "}.lst-kix_g3dvsik8v46c-8>li:before{content:"" counter(lst-ctn-kix_g3dvsik8v46c-8,lower-roman) ". "}.lst-kix_67q6k7r76ppj-4>li{counter-increment:lst-ctn-kix_67q6k7r76ppj-4}ol.lst-kix_5orfu8u579kf-7.start{counter-reset:lst-ctn-kix_5orfu8u579kf-7 0}.lst-kix_5orfu8u579kf-6>li:before{content:"" counter(lst-ctn-kix_5orfu8u579kf-6,decimal) ". "}.lst-kix_5orfu8u579kf-8>li:before{content:"" counter(lst-ctn-kix_5orfu8u579kf-8,lower-roman) ". "}ul.lst-kix_hwyt9y26yq4r-1{list-style-type:none}ol.lst-kix_eg70cn4trppt-7{list-style-type:none}ul.lst-kix_hwyt9y26yq4r-2{list-style-type:none}ol.lst-kix_eg70cn4trppt-8{list-style-type:none}ul.lst-kix_hwyt9y26yq4r-3{list-style-type:none}ul.lst-kix_hwyt9y26yq4r-4{list-style-type:none}ol.lst-kix_g3dvsik8v46c-0{list-style-type:none}.lst-kix_6myvsjmn8qag-1>li:before{content:"" counter(lst-ctn-kix_6myvsjmn8qag-1,lower-latin) ". "}ol.lst-kix_g3dvsik8v46c-1{list-style-type:none}ul.lst-kix_hwyt9y26yq4r-0{list-style-type:none}ol.lst-kix_g3dvsik8v46c-2{list-style-type:none}ol.lst-kix_g3dvsik8v46c-3{list-style-type:none}ol.lst-kix_g3dvsik8v46c-4{list-style-type:none}ol.lst-kix_eg70cn4trppt-0{list-style-type:none}ol.lst-kix_g3dvsik8v46c-5{list-style-type:none}ol.lst-kix_eg70cn4trppt-1{list-style-type:none}ol.lst-kix_g3dvsik8v46c-6{list-style-type:none}ol.lst-kix_eg70cn4trppt-2{list-style-type:none}ul.lst-kix_hwyt9y26yq4r-5{list-style-type:none}ol.lst-kix_g3dvsik8v46c-7{list-style-type:none}ol.lst-kix_eg70cn4trppt-3{list-style-type:none}ul.lst-kix_hwyt9y26yq4r-6{list-style-type:none}ol.lst-kix_g3dvsik8v46c-8{list-style-type:none}ol.lst-kix_eg70cn4trppt-4{list-style-type:none}ul.lst-kix_hwyt9y26yq4r-7{list-style-type:none}ol.lst-kix_eg70cn4trppt-5{list-style-type:none}ul.lst-kix_hwyt9y26yq4r-8{list-style-type:none}ol.lst-kix_5orfu8u579kf-6.start{counter-reset:lst-ctn-kix_5orfu8u579kf-6 0}ol.lst-kix_eg70cn4trppt-6{list-style-type:none}ol.lst-kix_doipf27yxj47-7.start{counter-reset:lst-ctn-kix_doipf27yxj47-7 0}.lst-kix_95tx3dl9s8o8-5>li{counter-increment:lst-ctn-kix_95tx3dl9s8o8-5}.lst-kix_g3dvsik8v46c-5>li{counter-increment:lst-ctn-kix_g3dvsik8v46c-5}.lst-kix_n02yq4vl24pk-5>li{counter-increment:lst-ctn-kix_n02yq4vl24pk-5}ol.lst-kix_n02yq4vl24pk-5.start{counter-reset:lst-ctn-kix_n02yq4vl24pk-5 0}.lst-kix_g3dvsik8v46c-2>li:before{content:"" counter(lst-ctn-kix_g3dvsik8v46c-2,lower-roman) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c16{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:124.9pt;border-top-color:#000000;border-bottom-style:solid}.c22{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:60pt;border-top-color:#000000;border-bottom-style:solid}.c34{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:87pt;border-top-color:#000000;border-bottom-style:solid}.c27{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:230.2pt;border-top-color:#000000;border-bottom-style:solid}.c25{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:52.5pt;border-top-color:#000000;border-bottom-style:solid}.c12{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:67.5pt;border-top-color:#000000;border-bottom-style:solid}.c1{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c0{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c5{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Times New Roman";font-style:normal}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c14{border-spacing:0;border-collapse:collapse;margin-right:auto}.c17{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c32{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c6{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c13{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c36{font-weight:700;font-size:14pt;font-family:"Times New Roman"}.c19{font-weight:400;font-size:10pt;font-family:"Arial"}.c35{color:#000000;text-decoration:none;vertical-align:baseline}.c3{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c21{font-weight:400;font-size:11pt;font-family:"Arial"}.c2{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c26{-webkit-text-decoration-skip:none;text-decoration:underline;text-decoration-skip-ink:none}.c8{font-size:10pt;font-family:"Times New Roman";font-weight:400}.c29{font-size:13pt;font-family:"Times New Roman";font-weight:700}.c24{background-color:#ffffff;max-width:499.7pt;padding:36pt 54pt 36pt 58.3pt}.c20{color:inherit;text-decoration:inherit}.c28{orphans:2;widows:2}.c33{width:33%;height:1px}.c10{margin-left:36pt;margin-right:36pt}.c11{padding:0;margin:0}.c23{font-style:italic}.c31{font-size:10pt}.c30{height:0pt}.c18{margin-left:36pt}.c7{height:11pt}.c9{text-indent:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c24"><div><p class="c15 c7"><span class="c6 c21"></span></p><p class="c7 c15"><span class="c6 c21"></span></p></div><p class="c32 c28"><span class="c6 c36">Comparison and Generation of YouTube Comments</span></p><p class="c32 c28"><span class="c6 c3">By: &#8203;Will Burford, Vincent Lin, Haoyu Sheng, Tongyu Zhou</span></p><p class="c4 c7"><span class="c0"></span></p><p class="c32 c28"><span class="c0">Abstract:</span></p><p class="c4 c10"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c6 c3">Consider a user or a company attempting to generate the most social impact through YouTube. What should they do to get the most likes? What are the determining factors that contribute to the popularity of the most-liked comments? The goal of our project is to develop a binary classifier that will determine which of two given Youtube comments will receive the most likes through features such as cooccurrences and sentiment polarity scores, and ultimately generate the YouTube comment that will get the most likes from a given video&rsquo;s metadata through methods such as Recurrent Neural Network and Markov Chain Bigram Model. The sentiment analysis classifier and co-occurrence classifier result in f1 scores of 0.51 and 0.68, and we are able to generate sensical phrases based on the category. </span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c4"><span class="c5">1. Introduction:</span></p><p class="c4"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c6 c3">We are seeking to create a system which will be able to produce the top comment on a trending YouTube video. This has two potential applications for industry. It would allow advertisers to insert advertisements into the comments of a YouTube video free of charge. This would also be a preferred location to put an ad because it would not be censored by an ad-blocker and is more likely to be read and interacted with by users than a banner. Additionally, it would allow YouTube to better understand which comments should be surfaced to users before a critical mass of comments is aggregated. This would allow a person viewing the video close to the time of release to still see the best comment available at that time.</span></p><p class="c4 c7"><span class="c6 c3"></span></p><p class="c4"><span class="c29">2. Survey of Existing Work(</span><span class="c29">s</span><span class="c5">):</span></p><p class="c4"><span class="c3">1. </span><span class="c3 c26">Retweeting Behavior Prediction</span><span class="c6 c3">: based on analysis of four features (user status, content, temporal, and social tie formation). Of these features, content and temporal are most relevant to us. Temporal analysis places emphasis on the time frame the tweet was created. Content analysis revealed that the inclusion of certain sentiment words that convey emotion (positive, neutral, negative) and a strong correlation between the user&rsquo;s profile and the topic of interest in the tweet (categorized using OpenCalais) garnered higher numbers of retweets. </span></p><ul class="c11 lst-kix_hwyt9y26yq4r-0 start"><li class="c1"><span class="c3">ChenHao Tan of Cornell University used a supervised learning approach, which generates several feature vectors of specific words and specific styles, such as user status and follower counts.</span><sup class="c3"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c6 c3">&nbsp;</span></li></ul><p class="c4"><span class="c3">2. </span><span class="c3 c26">Usefulness of Youtube Comments</span><span class="c3">: analyzing and predicting youtube comments and comment ratings&rdquo;.This paper analyzes the dependencies between Youtube comments, views, comment ratings and topic categories. In addition, they studied the influence of sentiment expressed in comments on the ratings for these comments using the SentiWordNet thesaurus, a lexical WordNet-based resource. Finally, to predict community acceptance for comments not yet rated, they built different classifiers for the estimation of ratings for these comments. </span><sup class="c3"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup></p><ul class="c11 lst-kix_3wy0zjmzx4j3-0 start"><li class="c1"><span class="c3">An undergraduate thesis &ldquo;Language of YouTube Video Comments&rdquo; determines certain supervised rules of youtube comments and their language form. It is possible to implement and formalize these rules into our youtube comment reader.</span><sup class="c3"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup></li></ul><p class="c4"><span class="c3">3. </span><span class="c3 c26">Writing Quality Predictor</span><span class="c3">: Annie Louis and Ani Nenkova used a classification model that built different feature vectors of models of writing.</span><sup class="c3"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup></p><p class="c4"><span class="c3">Because of the nature of YouTube comments, which are usually filled with vulgar languages and nonsensical grammars, there are very few established papers and these research only serve inspirational purposes. As a result, we decided to go our own way</span><sup class="c3"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c6 c3">. </span></p><p class="c4"><span class="c3 c23">Note: There are many other works that we&rsquo;ve surveyed and we embedded the works that are more relevant in our specific section of classification or generation. </span></p><p class="c4 c7"><span class="c0"></span></p><p class="c4"><span class="c29">3. </span><span class="c29">Formulation</span><span class="c5">:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 371.99px; height: 435.50px;"><img alt="" src="images/image2.png" style="width: 371.99px; height: 435.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c9"><span class="c6 c3">For the popularity classifier, we originally decided to build a Naive Bayes classifier to predict the number of likes each comment is likely to receive. However, due to the unbalanced nature of the dataset that most comments have zero likes, the classifier is very likely to overfit while only outputting 0. In order to avoid this conflict, we decided formulate the more likely to be liked comparator as a binary classification problem where our input consists of comment pairs. &nbsp;</span></p><p class="c4 c9"><span class="c3">For the comment generator, we </span><span class="c6 c3">represent it as an ngram language model and a deep learning problem. For the ngram language model, we apply smoothing and markov chain. For the deep learning model, we constructed a recurrent neural network, which takes in all the unique characters in the comment corpus, outputs a vector of the same length comprised of 0s&rsquo; and 1s&rsquo; to indicate which character is generated, and unrolls sequentially to output the word sequence.</span></p><p class="c4 c9"><span class="c3">The implementation details will be further discussed in the </span><span class="c2">Methodology</span><span class="c6 c3">&nbsp;section. </span></p><p class="c4 c7 c9"><span class="c6 c3"></span></p><p class="c4"><span class="c5">4. Methodology</span></p><p class="c4"><span class="c2">Classification:</span><span class="c2">&nbsp;</span><span class="c6 c3">We used an ensemble approach to classification. The two classifiers we used were co-occurrence and sentiment analysis. The details of each are covered below. We used a weighted voting process to combine the outputs of the classifiers, with co-occurrence weighted more highly because it has a significantly better f1-score. The data supplied to both classifiers is only the comment data on the specific video that we are generating for.</span></p><p class="c4"><span class="c2">Co-occurrence:</span></p><p class="c4"><span class="c2">Baseline:</span><span class="c6 c3">&nbsp;The initial approach to the co-occurrence model did not use any machine learning libraries. It took a single input comment, split the it into tokens, and went through all of the comments in the dataset to find the most similar existing comments based on co-occurrence similarity. It then averaged the likes from the most similar comments and assigned that number of likes to the input comment. This method did not perform very well because the data on likes was very sparse and it was common for a comment to get 0 likes as an output, which did not help when comparing two comments.</span></p><p class="c4"><span class="c2">Version 1:</span><span class="c6 c3">&nbsp;The next iteration of the co-occurrence model implemented a Naive Bayes classifier using a bag of words feature vector. The input was a single comment with its number of likes, and the classifier attempted to assign a number of likes to the new comment based on the data it had compiled by training on the training set. This model did not work because the classifier overfit to our dataset, which was filled with mostly 0 likes. For this reason, almost every input comment was 0 likes, which also was not useful for comparison between two comments, as we needed for deciding which generated comment would perform the best.</span></p><p class="c4"><span class="c2">Version 2:</span><span class="c6 c3">&nbsp;The final version of the co-occurrence classifier continues to use Naive Bayes on a bag of words feature vector. However, we changed our classification question to choose the more liked of two comments rather than assign a single comment a number of likes. Our feature vector is now two concatenated bags of words, the first section being the first comment and the second section being the second comment. The classifier was trained on a training set of the format (comment1, comment2, winner), where winner was either 0 if the first comment received more likes or 1 if the second comment did. We ran through every possible combination of comments in the training set during our training session, then we tested on every combination of comments in the testing set to compute the f1-score. When running to decide which which generated comment will receive more likes, we use pass through every combination of comments in the dataset for the given videos for training.</span></p><p class="c4"><span class="c0">Sentiment analysis:</span></p><p class="c4"><span class="c2">Baseline:</span><span class="c6 c3">&nbsp;Word-based + Naive Bayes + entire corpus</span></p><p class="c4 c9"><span class="c6 c3">The simplest version was based on the assumption that more positive comments would accordingly garner the most likes, so we performed word-based sentiment analysis by using a NLTK lexicon that already contains hundreds of thousands of sentiment-based words tagged with a polarity-based approach (assigned with either &ldquo;pos&rdquo;, &ldquo;neg&rdquo;, or &ldquo;neu&rdquo;). Using the top ten comments of each video, we first removed all stop words and then trained a Naive Bayes Classifier using the presence of positive words as a feature vector. Unfortunately, this did perform so well as we failed to take into account the actual nature of realistic youtube comments that generally was not so wholesome. </span></p><p class="c4"><span class="c2">Version 1:</span><span class="c6 c3">&nbsp;Sentence-based + Naive Bayes + per category</span></p><p class="c4 c9"><span class="c6 c3">Because word-based sentiment analysis took too long, required additional POS-tagging, and did not fully capture the general sentiment of the comment, we swapped to sentence-based sentiment analysis with VADER. This time, we do not remove stop words nor emoji as both are factored into the lexicon, which generates a compound score for each sentence. Instead of running the Naive Bayes classifier over the entire training corpus, we took the average sentiment scores per category, assuming that each category can be associated with an average comment sentiment score (for example, Gaming would have a much lower one than Lifestyle). This was also not as effective, however, as video comments were also heavily dependent on the particular fanbase that a Youtube creator gathered.</span></p><p class="c4"><span class="c2">Version 2</span><span class="c6 c3">: Numerical difference sentence based + Naive Bayes </span></p><p class="c4"><span class="c6 c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Instead of taking the sentiment scores for individual sentences, we decided to store the differences of these scores instead, matched with a value of 0 (first comment has more likes) or 1 (second comment has more likes) as the feature vector for the Naive Bayes classifier, as this would further reduce runtime because in training, we already performed the comparisons. The scope of the comments was reduced to multiple videos chosen randomly from the given category to correct for the disparities in category size. </span></p><p class="c4"><span class="c2">Follow up</span><span class="c6 c3">:</span></p><p class="c4 c9"><span class="c3">A further extension is to build on the work of Amy Madden, Ian Ruthven, and David McMenemy, who constructed a classification scheme for content analysis of Youtube comments, splitting each into one of 10 categories, under which there were many more subcategories.</span><sup class="c3"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c6 c3">&nbsp;Following their same strategy of deviant case analysis, we can associate sentiments with each category, which may be more informative than taking video categories since we are really in truth only interested in the content of the comments themselves. However, considering a large proportion of our corpus consisted of comments with no likes at all, with some whole videos falling into this category, and we only trained on comments with higher number of likes, we would probably not have enough training data to perform a meaningful analysis. Due to the limitations of the dataset, this model would theoretically work best if the likes per comment category followed some form of normal distribution. </span></p><p class="c4 c7 c9"><span class="c6 c3"></span></p><p class="c4"><span class="c0">Generation and Language Model:</span></p><p class="c4"><span class="c2 c23 c35">Data Preprocessing:</span></p><p class="c4"><span class="c6 c3">For our data processing, you are encouraged to enter in the tags, the title, and the category of the youtube video. From these informations, we create data file that contains all the comments from videos that have the same category and any keywords (no stop words) from the tags and the title. We then use these comments to generate our language model, which will produce the final comments. </span></p><p class="c4"><span class="c2">Baseline</span><span class="c6 c3">: Ngram model with no smoothing</span></p><p class="c4 c9"><span class="c6 c3">The simplest version of this language model is an Ngram model. After testing different number of grams, we decided to do trigrams for hopefully the best coherence. &nbsp;</span></p><p class="c4"><span class="c2">Version 1</span><span class="c6 c3">: Ngram model with smoothing with Katz backoff.</span></p><p class="c4 c9"><span class="c6 c3">We want to test the smoothing effects on the comprehensibility and the coherence of the text. It allows for more original generations, but it seems highly unlikely that that this will yield a more coherent text than that of our N-Gram Model. We predict that it will yield a worse coherence than the baseline version. </span></p><p class="c4"><span class="c2">Version 2:</span><span class="c6 c3">&nbsp; Generalized smoothing method based on likes.</span></p><p class="c4 c9"><span class="c6 c3">Since we want to generate comments with higher chances of likes, we want to give more weights to the comments with more likes. &nbsp;Our initial weight to give to the comments will be for every extra like, we will give one extra word for likes. So we will replicate each comments once per like so this will be smoothed for all language models. We will be trying out different weighting, such as multiples of 2, 3, 4 to find out the best weight.</span></p><p class="c4"><span class="c2">Version 3</span><span class="c6 c3">: Markov Chain Bigram model</span></p><p class="c4"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c6 c3">We used a Markov Chain to store the probabilities of transitioning from a present state to the next state using a bigram model primarily because working with trigrams and above would take too much time. We assume that all words that appear after a preceding word have the same probability, so we stored all the next states, including repetition, as values in a dictionary and used randomization to choose the next probable one. As the corpus increases, the model will naturally become more weighted accordingly to frequency. We believe this would be a better alternative to the previous language models as dynamic programming allows for more efficient space and time allocation. This improved the runtime by several minutes.</span></p><p class="c4"><span class="c2">Version 4:</span><span class="c6 c3">&nbsp;Markov Chain Model with a grammar rating model (FAILED took too long to run). </span></p><p class="c4"><span class="c3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We attempt to use the Stanford coreNLP parser to parse the comments into grammar trees. Then we take these grammar trees and count up same structures of the grammar tree. Then, we generate a probability for each grammatical rule. From these probability, we will rate the comments generated by our Ngram model and take the top 64 comments based on the probability of these comments. We took inspiration from papers attempting to combine the Ngram model and PCFG model. See.</span><sup class="c3"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup></p><p class="c4 c9"><span class="c6 c3">The primary reason why we didn&rsquo;t implement a PCFG model is because several assumptions of PCFG model cannot be assumed in the case of Youtube comments. First, most of the youtube comments are not in grammatical form. Second, the youtube comments will often be tokenized into words that are not correctly spelled. In other words, there will quite a few trolling comments. Both of these features of our dataset does not suit well for PCFG model since there will simply be too many forms of grammar in these Youtube comments. From the literatures we&rsquo;ve exam, the general consensus is that PCFG is worse than the NGram model for English </span></p><p class="c4"><span class="c2">Version 5</span><span class="c6 c3">: Markov Chain model with Named Entity Tagger.</span></p><p class="c4 c9"><span class="c6 c3">We noticed that that some of the comments generated contained Named Entities that are relevant to the title or the tags of the video. So we&rsquo;ve decided to replace the Named Entity generated in each comment generated with the Named Entities in certain comments. This is done through a probability generation weighting the Named Entity.</span></p><p class="c4"><span class="c2">Deep Learning Version</span><span class="c6 c3">: Character-based RNN Model.</span></p><p class="c4 c9"><span class="c3">We ran two versions of the Recurrent Neural Net Model: one basic version and another with LSTM. Both of these versions were implemented by Andrej Karpathy, which is included in the source code.</span><sup class="c3"><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup></p><p class="c4"><span class="c2">Vanilla RNN Version:</span><sup class="c2"><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup></p><p class="c4 c9"><span class="c6 c3">This model will take in the corpus and find the set of unique characters being used and use that to generate a binary feature features with one being the character that appears and zero being the character that does not appear. This cuts down drastically on the size of the matrices. It is unrealistic for us, who has very little computing power, to do word-based generation since there are at least 100,000 unique words in these youtube comments, which means a feature vector of length of at least 100,000.</span></p><p class="c4 c9"><span class="c3">In the Vanilla Version of this project, the neural network comprise of 2 hidden layers. With each layer of matrices and its weights being randomized initially and altered through a back-propagation function based on an entropy model. The back-propagation function alters the weights by calculating the derivative of weight of one node to the previous node and then adjust the values of the matrices to better match the contour of the data points. By altering the value of the matrices of each node, it will alter the weights of each features, which will change the value of the final value of the end node. For more see footnotes.</span><sup class="c3"><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup></p><p class="c4"><span class="c0">LSTM Version:</span></p><p class="c4 c9"><span class="c3">The LSTM Version added an Long Short Term Memory gate to it, which allows the extraction of long-term features of these vectors to be remembered better. Each node in the neural vectors, which contains different weights will be regulated by the LSTM, which will determine how much weight to give to each vectors. For example, the gender of the speaker will be difficult for the language model to capture in the its generation. However, since LSTM regulates the memory of the vectors. It&rsquo;ll do a better job of capturing these long term dependencies.</span><sup class="c3"><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup></p><p class="c4"><span class="c0">Evaluation Metric of Language Model:</span></p><p class="c4 c9"><span class="c6 c3">Since we are only generating 64 comments per language model. We will be using an extrinsic evaluation metric and rating it by hand by giving it a score of 0-5 for coherence for each file by our 4 group members! We chose not to use the perplexity to rate the language model since our data is extremely sparse and it is extremely clear which language model is superior.</span></p><p class="c4"><span class="c5">5. Experiment and Analysis:</span></p><p class="c4"><span class="c0">Experiment Setup: </span></p><p class="c4"><span class="c2">Dataset: </span><span class="c6 c3">Our dataset was obtained from Kaggle and contains information scraped from YouTube about trending videos at the time of collection. The dataset contains metadata for about 8,000 videos as well as around 700,000 comments with their corresponding like-counts. We found that this data was incredibly sparse as vast majority of comments received no likes.</span></p><p class="c4"><span class="c2">Preprocessing:</span><span class="c6 c3">&nbsp;Not much preprocessing was required for this dataset. Apart from dealing with non-ASCII characters, the data was already formatted in a made it easy to work with.</span></p><p class="c4"><span class="c2">Train/test sets: </span><span class="c3">We split our dataset into two parts: 70% for training and 30% for testing. We used these to calculate the f1-score for our classifier, but when we run our classifier on our generated comments we use the whole dataset as training data.</span></p><p class="c4"><span class="c6 c3">For the Comment generation portion of the corpus, we were correct in most of our predictions about the versions that we&rsquo;ve ran. The overall best model is the Markov Chain Bigram Model without Smoothing and with NER replacement. </span></p><p class="c4"><span class="c0">Classification:</span></p><p class="c4"><span class="c0">Scores:</span></p><p class="c32 c28"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 404.00px; height: 45.30px;"><img alt="" src="images/image3.png" style="width: 404.00px; height: 45.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c9"><span class="c6 c3">The above metrics is for the non-pairwise version of the sentiment analysis and the co-occurrence analysis. The top two values, 0.88 and 0.60, are the accuracies of our training set. Our f1 values for the classifiers are included as well. As we can see, the f1 for co-occurrence is 0 since comments with 0 like occupies around 60% of the dataset, which results in the overfitting classification of 0. </span></p><p class="c28 c32"><span class="c3">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 666.00px; height: 40.00px;"><img alt="" src="images/image1.png" style="width: 666.00px; height: 40.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c9"><span class="c6 c3">The top two values, 0.90 and 0.84, are the accuracies of our training set. Our f1 values for the classifiers are included as well. For the overall f1 score, we weighted the language model for co-occurrence heavier since it produced the comparatively higher f1 score. </span></p><p class="c4 c7"><span class="c6 c3"></span></p><p class="c32 c28"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 335.00px; height: 145.03px;"><img alt="" src="images/image4.png" style="width: 335.00px; height: 145.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c6 c3">Above is a simple illustration of the current performance of the classifier. It seems to capture the important phrases such as &ldquo;pewdie pie&rdquo; and how that correlates to the relative popularity of a comment instead of simply returning 0. </span></p><p class="c4 c7"><span class="c6 c3"></span></p><p class="c4"><span class="c2">Specific Examples:</span></p><a id="t.4862d734ffa975f4f66dd9b755ee9d87acbe7d14"></a><a id="t.0"></a><table class="c14"><tbody><tr class="c30"><td class="c27" colspan="1" rowspan="1"><p class="c13"><span class="c0">Comments</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c13"><span class="c0">Sentiment</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c13"><span class="c0">Co-occurrence</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c13"><span class="c0">Ensemble</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c13"><span class="c0">Correct</span></p></td></tr><tr class="c30"><td class="c27" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">I forgive you</span></p><p class="c13"><span class="c6 c3">Vs.</span></p><p class="c13"><span class="c6 c3">I don&rsquo;t have respect for you</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">0</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">0</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">0</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">1</span></p></td></tr><tr class="c30"><td class="c27" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">Tf did he say??</span></p><p class="c13"><span class="c6 c3">Vs.</span></p><p class="c13"><span class="c6 c3">to what</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">1</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c13"><span class="c3 c6">1</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">1</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">0</span></p></td></tr><tr class="c30"><td class="c27" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">Like if agree</span></p><p class="c13"><span class="c6 c3">Vs.</span></p><p class="c13"><span class="c6 c3">Well I accept your apology Mr PewDiePie. It just slipped out. You are still a good YouTuber</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">1</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">1</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">1</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c13"><span class="c6 c3">1</span></p></td></tr></tbody></table><p class="c4 c9"><span class="c6 c3">Here, we see in all cases that the sentiment classifier is picking the generally more positive sentiment comment. The fact that the sentiment classifier is not always correct shows us that our assumption that a more positive comment receives more likes does not hold 100% of the time. The co-occurrence model incorrectly classified the first two comment pairs. This is likely because the comment pairs that we had used for training generally had a comment more like &nbsp;&ldquo;I forgive you&rdquo; or &ldquo;to what&rdquo; winning the dual over a comment such as &ldquo;I don&rsquo;t have respect for you&rdquo; or &ldquo;Tf did he say??&rdquo; respectively.</span></p><p class="c4 c7"><span class="c0"></span></p><p class="c4 c7"><span class="c0"></span></p><p class="c4 c7"><span class="c0"></span></p><p class="c4 c7"><span class="c0"></span></p><p class="c4"><span class="c0">Generation:</span></p><p class="c4"><span class="c2">Scores: </span><span class="c6 c3">Rating of Language Model:</span></p><a id="t.8fceb639bbf3deec83c174d78d21cd30c1f2f23c"></a><a id="t.1"></a><table class="c14"><tbody><tr class="c30"><td class="c16" colspan="1" rowspan="1"><p class="c13"><span class="c0">Baseline:</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c13"><span class="c0">Markov Bigram:</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c13"><span class="c0">Markov Trigram:</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c13"><span class="c0">Deep Learning:</span></p></td></tr><tr class="c30"><td class="c16" colspan="1" rowspan="1"><p class="c13"><span class="c0">2.5</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c13"><span class="c0">4.5</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c13"><span class="c0">4.25</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c13"><span class="c0">3.5</span></p></td></tr></tbody></table><p class="c4"><span class="c0">Ngram Model:</span></p><p class="c4"><span class="c3">When we first viewed the comments generated in the Ngram model, we were in disbelief at how incoherent and illogical most of these comments were. As it turns out, we tested our language model on the shakespeare corpus and it yielded substantially better results. So we digged into our data set and read many Youtube Comments and we discovered that data set contains the following problems:</span><sup class="c3"><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup></p><ol class="c11 lst-kix_g3dvsik8v46c-0 start" start="1"><li class="c1"><span class="c6 c3">Lack of Well Liked Comments:</span></li></ol><p class="c4 c18"><span class="c3">Since this dataset is scrapped at 12:00am everyday and for the top 200 trending videos, there are often times numerous comments that are not liked. The majority of the comments are of 0 likes and the ones that are liked are usually 1 or 2 likes. This resulted in our </span><span class="c2">Well-Like Smoothing feature</span><span class="c6 c3">&nbsp;producing results that are not significant in its generation.</span></p><ol class="c11 lst-kix_g3dvsik8v46c-0" start="2"><li class="c1"><span class="c6 c3">Word and Sentence Sparsity and Trolling Comments:</span></li></ol><p class="c4 c18"><span class="c6 c3">This includes a repetition of certain derogatory words over and over again. Our record is one person repeated a racial slur 325 times in one comment, which lead us to only take the first 20 words of every comments in our corpus generation. Most of the comments are in different forms and there are many different sentences with little recurrence such the bigram probability of each sentences are quite evenly distributed on the lower end. This signals that our dataset is actually not largely enough to generate the model properly.</span></p><ol class="c11 lst-kix_g3dvsik8v46c-0" start="3"><li class="c1"><span class="c6 c3">Grammatical incorrectness:</span></li></ol><p class="c4 c18"><span class="c6 c3">Most comments on youtube contains incorrect spellings and an abundance of punctuation marks, such as &hellip;&hellip;!!!!!!. Some of the comments also skip several lines and contain absurd drawings based on unique characters and emojis. Furthermore, some of the comments contains incorrect grammar. These features of the youtube comments make the comments to be extremely random. This factored into why we decided to remove version 4.</span></p><p class="c4"><span class="c0">Smoothing with Katz Backoff Feature:</span></p><p class="c4"><span class="c6 c3">As predicted, smoothing decreases the coherence of the sentence generated. This did increase the diversity of words by making the comments generated more incomprehensible (which we did not think it was initially possible).</span></p><p class="c4"><span class="c0">NER replacement feature:</span></p><p class="c4"><span class="c6 c3">Our NER replacement feature Tagger was extremely beneficial. However, for some NER, because the grammar was messed up, the POS-tagging layer of NER tagger could not tag it correctly. Furthermore, some of the named entities and comments are in the transcript of the video, which we could not obtain. Thus, some of NER are incorrectly placed in the context of the video. For example, in the appendix, the NER tagger replaced England incorrectly Hart in line 9.</span></p><p class="c4"><span class="c2">Version 4 Failure: </span><span class="c6 c3">Our original idea for the grammar rating model is to use NLTK&rsquo;s Viterbi parser to generate probabilities based on all possible grammar trees of a particular sentence. From these probabilities, we would return sentences that generated the top probabilities. However, in order to create a grammar structure that was compatible with the format that NLTK accepts, we needed to parse from a treebank file that would take too long to run through all the comments of the videos. </span></p><p class="c4"><span class="c0">Deep Learning Model:</span></p><p class="c4"><span class="c3">For the Deep Learning Model, we ran through 20110 iterations of the vanilla model of RNN, which took around 5 hours on CPU and 63000 iterations of the LSTM version of RNN, which took around 19 hours on the CPU. The vanilla version plateaued at a training loss of 44% and the LSTM version plateaued at a training loss of 32%. Screenshots of the result are included in the appendix. When we look at the results for both deep learning model, once again the misspellings and punctuations repetition really decreased the fluency and coherence of the language model. However, if we spell-checked and remove all punctuations, this language model would perfectly suffice. In the future, we would want to run this model on a computer with a dedicated GPU and test out different hyperparameters and backpropagation functions in order to get a better result. Note, we were unable to run the NER changer and Grammar rating model since most of the words are misspelled and often makes no sense.</span></p><p class="c4"><span class="c0">Conclusion: </span></p><p class="c4"><span class="c3">We were surprised at the progress we made in terms of an f1-score for predicting comment likes with solely using the assumption that the more positive comment will receive more likes. When we combined this with the bag of words model, we saw that our f1-score increased more. One of the major weakness of the project was the dataset we were using. The likes on the comments were very sparse, with whole comment sections only having a few likes at times. If we were to do this again, we would make sure to scrape our own data from YouTube so that we could ensure a higher density of like data.</span></p><p class="c4"><span class="c6 c3">We were unsurprised that markov chain bigram model got the best score as the nature of our sparse data mentioned. An extension that we would work on in the future in to successfully create and test Version 4 of our data. Furthermore, we would like to explore the RNN language model by testing out different hyperparameters and backpropagation functions in order to get a better result.</span></p><p class="c4"><span class="c3">In future work, we would like to extend this work to predicting similar comment-like relationships. This could be on other platforms such as Facebook or Instagram. On a platform such as Facebook, the program could even be extended so that it would be able to predict the number of likes on an original post.</span></p><p class="c4 c7"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c4 c7"><span class="c0"></span></p><p class="c4"><span class="c0">Appendix:</span></p><p class="c4"><span class="c0">Contributions:</span></p><p class="c4"><span class="c6 c3">Haoyu worked on both classifiers, data handling, and is very proud of the GUI that he made.</span></p><p class="c4"><span class="c6 c3">Tongyu worked on the sentiment classifier and the language model versions of the generation.</span></p><p class="c4"><span class="c6 c3">Vincent worked on all versions of language model and deep learning generation.</span></p><p class="c4"><span class="c6 c3">Will worked on the co-occurrence classifier, dataset handling / validation, and combining the generation with the classifiers into a unified system.</span></p><p class="c4 c7"><span class="c0"></span></p><p class="c4"><span class="c0">Top 10 Generated Comments for Language Model:</span></p><p class="c4"><span class="c0">Bigram Markov Chain Model:</span></p><ol class="c11 lst-kix_67q6k7r76ppj-0 start" start="1"><li class="c1"><span class="c6 c3">&#x1f448; watch by the best Vlog was a great video, but I love Youtube for you so much someday I</span></li><li class="c1"><span class="c6 c3">Omg he wouldn&#39;t be ur friend anymore</span></li><li class="c1"><span class="c6 c3">PEWDIEPIE accidentally played song with words nig%%r during his first I LOVE! You should be able </span></li><li class="c1"><span class="c6 c3">NO HONEY NOOO can&#39;t believe that part on Trending let&#39;s go</span></li><li class="c1"><span class="c6 c3">dab away</span></li><li class="c1"><span class="c6 c3">oh wait this vid is that so much &#x1f60d;&#x1f48b;&#x1f499;&#x1f44d; Bro y didnt u for the</span></li><li class="c1"><span class="c6 c3">all 365 vlogs so annoying. Like its </span></li><li class="c1"><span class="c6 c3">really make my day &#8252;&#65039;&#8252;&#65039;&#8252;&#65039; a mini Logan Paul</span></li><li class="c1"><span class="c6 c3">Pauls please stop saying they&#39;re the real world you just taking ur boy 2</span></li><li class="c1"><span class="c6 c3">aw geez rick this vlog to meet you can do</span></li></ol><p class="c4 c7"><span class="c0"></span></p><p class="c4"><span class="c2">Bigram Markov Chain with NER: </span><span class="c6 c3">The title of the video is &ldquo;Florida Man Stole Kevin Hart&#39;s Attention at the NYC Marathon.&rdquo;</span></p><ol class="c11 lst-kix_6myvsjmn8qag-0 start" start="1"><li class="c1"><span class="c6 c3">everyone loves it SUBSCRIBE will see</span></li><li class="c1"><span class="c6 c3">the top comments? lets see it SUBSCRIBE kill</span></li><li class="c1"><span class="c6 c3">play button &#x1f48e;&#x1f606;&#x1f606;&#x1f606;&#x1f606;&#x1f606;&#x1f606;&#x1f601;&#x1f601;&#x1f601;&#x1f601;&#x1f601;&#x1f601;&#x1f601;</span></li><li class="c1"><span class="c6 c3">Florida You think this isn&#39;t trump sure looks like H&#x1f525;LLLLLLZZZZ YEEEAAAHHHH</span></li><li class="c1"><span class="c6 c3">dying when he left when you from</span></li><li class="c1"><span class="c6 c3">Happy one year vlogaversary all 365</span></li><li class="c1"><span class="c6 c3">girl with or fuck you</span></li><li class="c1"><span class="c6 c3">from the money. We don&#39;t have the money. We don&#39;t even have seen</span></li><li class="c1"><span class="c6 c3">i&#39;ve just dab away subscribed today. This was dying when</span></li><li class="c1"><span class="c3">&amp; Florida not funny watching you from Hart can&#39;t </span></li></ol><p class="c4 c7"><span class="c6 c3"></span></p><p class="c4"><span class="c0">Deep Learning LSTM Version:</span></p><ol class="c11 lst-kix_eg70cn4trppt-0 start" start="1"><li class="c1"><span class="c6 c3">That so hello something</span></li><li class="c1"><span class="c6 c3">Celo martined??????</span></li><li class="c1"><span class="c6 c3">&#x1f602;&#x1f602;&#x1f602;</span></li><li class="c1"><span class="c6 c3">Who else u playwfoid face when watching your gender skin cooking on fku delusional! Stephen come just Big time, come to Blade Runner twist, you&#39;re just a chains descrite prote snoleemes from phones, but, please like hem? These tho&#x1f600; thing, I found it coming. Lol do they promige that then &ldquo;satisfying*</span></li><li class="c1"><span class="c6 c3">Avery to put your friend! but i think she&#39;s cut on muvin bang to advort</span></li><li class="c1"><span class="c6 c3">Kinda mustache : sad, makeup there.indeven sounds unin the own jewish going as</span></li><li class="c1"><span class="c6 c3">Learn how to serve use its Poline</span></li><li class="c1"><span class="c6 c3">Oh mutine, BL to mind set numbers, deviance of the other? It&#39;s not a extrempo</span></li><li class="c1"><span class="c6 c3">this was just looking forem</span></li><li class="c1"><span class="c6 c3">Keep up weight for everything about scamsey, how the play points about the inversulations.</span></li></ol><div><p class="c4 c7"><span class="c6 c21"></span></p></div><hr class="c33"><div><p class="c13 c28"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c31">&nbsp;</span><span class="c8">&nbsp;</span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=https://chenhaot.com/pubs/wording-effects-message-propagation.pdf&amp;sa=D&amp;ust=1514621863558000&amp;usg=AFQjCNF8R8S4GdqEZaifit5mBZB6j6sxCQ">https://chenhaot.com/pubs/wording-effects-message-propagation.pdf</a></span></p></div><div><p class="c13 c28"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c31">&nbsp;</span><span class="c8">&nbsp;</span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=https://dl.acm.org/citation.cfm?id%3D1772781&amp;sa=D&amp;ust=1514621863559000&amp;usg=AFQjCNEtljtkodCZKyuSIJQqWwr4EGTutA">https://dl.acm.org/citation.cfm?id=1772781</a></span></p></div><div><p class="c13 c28"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c31">&nbsp;</span><span class="c8">&nbsp;</span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=https://is.muni.cz/th/209464/ff_b/Varga_Thesis.pdf&amp;sa=D&amp;ust=1514621863560000&amp;usg=AFQjCNHsceradS5gslSCcFECgm1SVOS2tw">https://is.muni.cz/th/209464/ff_b/Varga_Thesis.pdf</a></span></p></div><div><p class="c13 c28"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c31">&nbsp;</span><span class="c8">&nbsp;</span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=http://www.aclweb.org/anthology/Q13-1028&amp;sa=D&amp;ust=1514621863560000&amp;usg=AFQjCNHZtYhQS2MJ-dn6Jj06aWs5c873Gg">http://www.aclweb.org/anthology/Q13-1028</a></span></p></div><div><p class="c13 c28"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3D7623dHG9_q0&amp;sa=D&amp;ust=1514621863564000&amp;usg=AFQjCNEIwxyHUaGX6_u1g0erWCta_mF-Dw">&nbsp;https://www.youtube.com/watch?v=7623dHG9_q0</a></span></p></div><div><p class="c13 c28"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c8">&nbsp;</span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=https://strathprints.strath.ac.uk/42033/&amp;sa=D&amp;ust=1514621863564000&amp;usg=AFQjCNHePRUa1hT7MjHfIk4_gkUPUDKS4A">https://strathprints.strath.ac.uk/42033/</a></span></p></div><div><p class="c13 c28"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c31">&nbsp;</span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=https://pdfs.semanticscholar.org/adbb/58b693ba0211aac392fb2c49a71c8eecfa7a.pdf&amp;sa=D&amp;ust=1514621863557000&amp;usg=AFQjCNHAOH28MH0-QbgoFjhaPJ6FtAAzig">https://pdfs.semanticscholar.org/adbb/58b693ba0211aac392fb2c49a71c8eecfa7a.pdf</a></span><span class="c8">&nbsp;and </span><span class="c8 c17"><a class="c20" href="https://www.google.com/url?q=http://www.aclweb.org/anthology/P94-1011&amp;sa=D&amp;ust=1514621863558000&amp;usg=AFQjCNGGjqSZ4y233yX5_KkaS6UowkZQ-Q">http://www.aclweb.org/anthology/P94-1011</a></span></p></div><div><p class="c13 c28"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c31">&nbsp;</span><span class="c8">Karpthy&rsquo;s talk on RNN and his implementations of Character-based RNN Model with LSTM </span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks&amp;sa=D&amp;ust=1514621863561000&amp;usg=AFQjCNGbZ1edAHLvocuGsRKRcIK2EwJfQg">https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks</a></span></p></div><div><p class="c13 c28"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c31">&nbsp;</span><span class="c8">Basic explanations of RNN </span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=http://karpathy.github.io/2015/05/21/rnn-effectiveness/&amp;sa=D&amp;ust=1514621863562000&amp;usg=AFQjCNExQlgUVsumgv62QCloYyTiKq3gYw">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></span><span class="c6 c8">&nbsp;</span></p></div><div><p class="c13 c28"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span class="c31">&nbsp;</span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=https://arxiv.org/pdf/1308.0850.pdf&amp;sa=D&amp;ust=1514621863563000&amp;usg=AFQjCNGS_yrTdqyeo0Pcih-jbCUwxx7Eaw">https://arxiv.org/pdf/1308.0850.pdf</a></span><span class="c6 c8">&nbsp;</span></p></div><div><p class="c13 c28"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span class="c8">&nbsp;For more informations, see </span><span class="c17 c8"><a class="c20" href="https://www.google.com/url?q=http://colah.github.io/posts/2015-08-Understanding-LSTMs/&amp;sa=D&amp;ust=1514621863561000&amp;usg=AFQjCNFn6Kxgf7On0Rs5RVJUHfgw9oV8yQ">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></span><span class="c6 c8">&nbsp;which explains LSTM very well. </span></p></div><div><p class="c13 c28"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span class="c31">&nbsp;For research and entertainment purposes, this article enlightened us on the methodology of Youtube comments being upvoted, the lack of oversight and moderation and the encouragement of trolling by various communities. </span><span class="c17 c31"><a class="c20" href="https://www.google.com/url?q=https://www.newstatesman.com/science-tech/internet/2016/10/why-are-youtube-comments-worst-internet&amp;sa=D&amp;ust=1514621863563000&amp;usg=AFQjCNE9BkQXU2qsrJX_eIuacAPPZTY-yg">https://www.newstatesman.com/science-tech/internet/2016/10/why-are-youtube-comments-worst-internet</a></span></p></div></body></html>